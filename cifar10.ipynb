{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import  SummaryWriter\n",
        "import einops\n",
        "writer=SummaryWriter('logs')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000\n",
            "torch.Size([3, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "transform=torchvision.transforms.Compose([torchvision.transforms.RandAugment(num_ops=2,magnitude=6),torchvision.transforms.ToTensor()])\n",
        "cifar_trainset=torchvision.datasets.CIFAR10('CIFAR10',train=True,transform=transform,download=True)\n",
        "cifar_testset=torchvision.datasets.CIFAR10('CIFAR10',train=False,transform=torchvision.transforms.ToTensor(),download=True)\n",
        "print(len(cifar_trainset))\n",
        "cifar_trainset,cifar_validateset=torch.utils.data.random_split(cifar_trainset,[40000,10000])\n",
        "load_trainset=DataLoader(cifar_trainset,batch_size=64,shuffle=True,drop_last=True)\n",
        "load_validateset=DataLoader(cifar_validateset,batch_size=64,shuffle=True,drop_last=True)\n",
        "load_testset=DataLoader(cifar_testset,batch_size=64,drop_last=False)\n",
        "print(cifar_testset[0][0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class residual_connection(nn.Module):\n",
        "    def __init__(self,in_channel,hidden_channel,out_channel,kernel_size=3,stride=1,padding=1):\n",
        "        super().__init__()\n",
        "        self.conv1=nn.Conv2d(in_channel,hidden_channel,kernel_size=1,stride=1,padding=0)\n",
        "        self.conv2=nn.Conv2d(hidden_channel,hidden_channel,kernel_size,padding=padding)\n",
        "        self.conv3=nn.Conv2d(hidden_channel,out_channel,kernel_size=1,padding=0)\n",
        "        self.silu=nn.SiLU()\n",
        "        self.batchnorm1=nn.BatchNorm2d(in_channel)\n",
        "        self.batchnorm2=nn.BatchNorm2d(hidden_channel)\n",
        "        self.batchnorm3=nn.BatchNorm2d(hidden_channel)\n",
        "    def forward(self,x):\n",
        "        y=x.clone()\n",
        "        x=self.batchnorm1(x)\n",
        "        x=self.conv1(x)\n",
        "        x=self.silu(x)\n",
        "        x=self.batchnorm2(x)\n",
        "        x=self.conv2(x)\n",
        "        x=self.silu(x)\n",
        "        x=self.batchnorm3(x)\n",
        "        x=self.conv3(x)\n",
        "        x=self.silu(x)\n",
        "        x=x+y\n",
        "        return x\n",
        "class resnetblock(nn.Module):\n",
        "    def __init__(self,in_channel,hidden_channel,out_channel,num_residual_connection=2,kernel_size=3,stride=1,padding=1):\n",
        "        super().__init__()\n",
        "        self.residual_connections=nn.ModuleList([residual_connection(in_channel,hidden_channel,out_channel,kernel_size,stride,padding) for _ in range(num_residual_connection)])\n",
        "    def forward(self,x):\n",
        "        for residual_connection in self.residual_connections:\n",
        "            x=residual_connection(x)\n",
        "        return x\n",
        "class resnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.batchnorm1 = nn.BatchNorm2d(3)\n",
        "        self.conv1 = nn.Conv2d(3, 32, 5, padding=2)\n",
        "        self.resnetblock1=resnetblock(32,16,32,2,5,1,padding=2)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.silu1 = nn.SiLU()\n",
        "\n",
        "        # self.batchnorm2 = nn.BatchNorm2d(32)\n",
        "        # self.conv2 = nn.Conv2d(32, 32, 5, padding=2)\n",
        "        self.resnetblock2=resnetblock(32,16,32,2,5,1,padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.silu2 = nn.SiLU()\n",
        "\n",
        "        self.batchnorm3 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "        self.resnetblock3=resnetblock(64,16,64,2,5,1,padding=2)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.silu3 = nn.SiLU()\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 16, 64)\n",
        "        self.silu4 = nn.SiLU()\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "        \n",
        "    #     # 初始化权重为 Xavier\n",
        "    #     self._initialize_weights()\n",
        "    \n",
        "    # def _initialize_weights(self):\n",
        "    #     \"\"\"使用 Xavier 初始化所有卷积层和全连接层的权重\"\"\"\n",
        "    #     for m in self.modules():\n",
        "    #         if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "    #             nn.init.xavier_normal_(m.weight)\n",
        "    #             if m.bias is not None:\n",
        "    #                 nn.init.constant_(m.bias, 0)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x=self.batchnorm1(x)\n",
        "        x=self.conv1(x)\n",
        "        x=self.resnetblock1(x)\n",
        "        x=self.pool1(x)\n",
        "        x=self.silu1(x)\n",
        "        # x=self.batchnorm2(x)\n",
        "        x=self.resnetblock2(x)\n",
        "        x=self.pool2(x)\n",
        "        x=self.silu2(x)\n",
        "        x=self.batchnorm3(x)\n",
        "        x=self.conv3(x)\n",
        "        x=self.resnetblock3(x)\n",
        "        x=self.pool3(x)\n",
        "        x=self.silu3(x)\n",
        "        x=self.flatten(x)\n",
        "        x=self.silu4(self.fc1(x))\n",
        "        x=self.fc2(x)\n",
        "        return x\n",
        "model=resnet()\n",
        "model=model.to(device)\n",
        "test=torch.randn(2,3,32,32,device=device)\n",
        "print(model(test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([0.1302, 0.0539, 0.0815, 0.1872,    nan,    nan, 0.0057, 0.0472,    nan,\n",
            "        0.1805], device='cuda:0'), tensor(0.1171, device='cuda:0'), tensor([0.1423, 0.0989, 0.1429, 0.1138, 0.0000,    nan, 0.0612, 0.0990,    nan,\n",
            "        0.1168], device='cuda:0'), tensor([0.1200, 0.0370, 0.0570, 0.5260, 0.0000, 0.0000, 0.0030, 0.0310, 0.0000,\n",
            "        0.3970], device='cuda:0'))\n"
          ]
        }
      ],
      "source": [
        "# 从 metric.py 导入 get_metrics 函数\n",
        "# 如果没有 metric.py，可以使用下面的函数定义\n",
        "def get_metrics(model,load_testset,beta=1,fault_tolerance=0):\n",
        "    acc=torch.zeros(10,device=device)\n",
        "    predict=torch.zeros(10,device=device)\n",
        "    total=torch.zeros(10,device=device)\n",
        "    if fault_tolerance:\n",
        "        for data in load_testset:\n",
        "            imgs,label=data\n",
        "            imgs=imgs.to(device)\n",
        "            label=label.to(device)\n",
        "            ans=model(imgs)\n",
        "            for i in range(10):\n",
        "                acc[i]+=torch.sum(torch.sum(torch.topk(ans,1+fault_tolerance,dim=1).indices==label.unsqueeze(1),dim=-1)*(label==i)).item()\n",
        "                total[i]+=torch.sum(label==i).item()\n",
        "                predict[i]+=torch.sum(torch.topk(ans,1+fault_tolerance,dim=1).indices==i).item()\n",
        "    else:\n",
        "        for data in load_testset:\n",
        "            imgs,label=data\n",
        "            imgs=imgs.to(device)\n",
        "            label=label.to(device)\n",
        "            ans=model(imgs)\n",
        "            for i in range(10):\n",
        "                acc[i]+=torch.sum((torch.argmax(ans,axis=1)==label)*(label==i)).item()\n",
        "                total[i]+=torch.sum(label==i).item()\n",
        "                predict[i]+=torch.sum(torch.argmax(ans,axis=1)==i).item()\n",
        "    precision=acc/predict\n",
        "    recall=acc/total\n",
        "    f1=precision*recall/(beta**0.5*precision+recall)*(1+beta**0.5)\n",
        "    accuracy=torch.sum(acc)/torch.sum(total)\n",
        "    return f1,accuracy,precision,recall\n",
        "\n",
        "print(get_metrics(model,load_testset,fault_tolerance=0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class cifarmodel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.batchnorm1 = nn.BatchNorm2d(3)\n",
        "        self.conv1 = nn.Conv2d(3,32,5,padding=2)\n",
        "        self.silu1 = nn.SiLU()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        \n",
        "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32,32,5,padding=2)\n",
        "        self.silu2 = nn.SiLU()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        \n",
        "        self.batchnorm3 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32,64,5,padding=2)\n",
        "        self.silu3 = nn.SiLU()\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        \n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64*16,64)\n",
        "        self.silu4 = nn.SiLU()\n",
        "        self.fc2 = nn.Linear(64,10)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.silu1(x)\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.silu2(x)\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = self.batchnorm3(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.silu3(x)\n",
        "        x = self.pool3(x)\n",
        "        \n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.silu4(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "model=cifarmodel()\n",
        "model=model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self,patch1=4,patch2=4,channel=3):\n",
        "        super().__init__()\n",
        "        self.patch1=patch1\n",
        "        self.patch2=patch2\n",
        "        self.tokenize=nn.Linear(patch1*patch2*channel,64)\n",
        "        self.embedding=nn.parameter.Parameter(torch.randn(65,64,device=device),requires_grad=True)\n",
        "        self.classembedding=nn.parameter.Parameter(torch.randn(64,device=device),requires_grad=True)\n",
        "        self.transformer=nn.TransformerEncoder(nn.TransformerEncoderLayer(64,4,dim_feedforward=128,batch_first=True),num_layers=3)\n",
        "        self.fc=nn.Linear(64,10)\n",
        "    def forward(self,x):\n",
        "        x=einops.rearrange(x,'batch channel (h patch1) (w patch2) -> batch channel h w (patch1 patch2)',patch1=self.patch1,patch2=self.patch2)\n",
        "        x=einops.rearrange(x,'batch channel h w patch -> batch (h w) (channel patch)')\n",
        "        x=self.tokenize(x)\n",
        "        x=torch.cat((self.classembedding.unsqueeze(0).unsqueeze(0).expand(x.shape[0],-1,-1),x),dim=1)\n",
        "        x=x+self.embedding.unsqueeze(0).expand(x.shape[0],-1,-1)\n",
        "        x=self.transformer(x)\n",
        "        x=self.fc(x[:,0])\n",
        "        return x\n",
        "model=ViT()\n",
        "model=model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=0,loss=1291.5514106750488,f1=nan,acc=0.18699920177459717\n",
            "epoch=1,loss=1113.1976212263107,f1=0.36242586374282837,acc=0.38982370495796204\n",
            "epoch=2,loss=925.9437798261642,f1=0.514957070350647,acc=0.5205328464508057\n",
            "epoch=3,loss=810.2734661102295,f1=0.510861337184906,acc=0.528245210647583\n",
            "epoch=4,loss=729.5858160853386,f1=0.5992826819419861,acc=0.6025640964508057\n"
          ]
        }
      ],
      "source": [
        "optimizer=torch.optim.AdamW(model.parameters(),lr=0.01)\n",
        "loss=nn.CrossEntropyLoss()\n",
        "loss=loss.to(device)\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    totalloss=0\n",
        "    for data in load_trainset:\n",
        "        imgs,label=data\n",
        "        imgs=imgs.to(device)\n",
        "        label=label.to(device)\n",
        "        result=model(imgs)\n",
        "        result_loss=loss(result,label)\n",
        "        optimizer.zero_grad()\n",
        "        result_loss.backward()\n",
        "        optimizer.step()\n",
        "        totalloss+=result_loss.item()\n",
        "    print(f\"epoch={epoch},loss={totalloss},\",end='')\n",
        "    model.eval()\n",
        "    writer.add_scalar(tag='loss',scalar_value=totalloss,global_step=epoch)\n",
        "    with torch.no_grad():\n",
        "        f1,acc,precision,recall=get_metrics(model,load_validateset)\n",
        "        print(f\"f1={f1.mean()},acc={acc}\")\n",
        "        writer.add_scalar(tag='f1',scalar_value=f1.mean(),global_step=epoch)\n",
        "        writer.add_scalar(tag='acc',scalar_value=acc,global_step=epoch)\n",
        "writer.close()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pt",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
